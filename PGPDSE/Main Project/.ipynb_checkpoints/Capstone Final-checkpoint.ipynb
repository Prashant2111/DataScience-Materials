{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from textblob) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "mysql-connector-python 8.0.13 requires protobuf>=3.0.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Text Mining\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "\n",
    "#Topic Modelling\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Others\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Files\n",
    "\n",
    "fb_obama = pd.read_csv('Facebook_Obama.csv')\n",
    "gp_obama = pd.read_csv('GooglePlus_Obama.csv')\n",
    "li_obama = pd.read_csv('LinkedIn_Obama.csv')\n",
    "\n",
    "fb_economy = pd.read_csv('Facebook_Economy.csv')\n",
    "gp_economy = pd.read_csv('GooglePlus_Economy.csv')\n",
    "li_economy = pd.read_csv('LinkedIn_Economy.csv')\n",
    "\n",
    "final = pd.read_csv('News_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Palestine and Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93239, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data based on the topics required\n",
    "temp1 = np.where((final['Topic']=='palestine'))[0]   #index of Palestine\n",
    "temp2 = np.where(final['Topic']=='microsoft')[0]     #index of microsoft\n",
    "print(len(temp1)+len(temp2))\n",
    "final.shape #initial shape  -- final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62538, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.drop(temp1,axis=0,inplace=True)   #Dropping Palestine\n",
    "final.drop(temp2,axis=0,inplace=True)   #Dropping Microsoft\n",
    "final.shape #final shape  -- final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDLink               float64\n",
       "Title                 object\n",
       "Headline              object\n",
       "Source                object\n",
       "Topic                 object\n",
       "PublishDate           object\n",
       "SentimentTitle       float64\n",
       "SentimentHeadline    float64\n",
       "Facebook               int64\n",
       "GooglePlus             int64\n",
       "LinkedIn               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99248.0</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemetery</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemete...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>obama</td>\n",
       "      <td>2002-04-02 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0533</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IDLink                                             Title  \\\n",
       "0  99248.0  Obama Lays Wreath at Arlington National Cemetery   \n",
       "\n",
       "                                            Headline     Source  Topic  \\\n",
       "0  Obama Lays Wreath at Arlington National Cemete...  USA TODAY  obama   \n",
       "\n",
       "           PublishDate  SentimentTitle  SentimentHeadline  Facebook  \\\n",
       "0  2002-04-02 00:00:00             0.0            -0.0533        -1   \n",
       "\n",
       "   GooglePlus  LinkedIn  \n",
       "0          -1        -1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99248.0</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemetery</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemete...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>obama</td>\n",
       "      <td>2002-04-02 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0533</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IDLink                                             Title  \\\n",
       "0  99248.0  Obama Lays Wreath at Arlington National Cemetery   \n",
       "\n",
       "                                            Headline     Source  Topic  \\\n",
       "0  Obama Lays Wreath at Arlington National Cemete...  USA TODAY  obama   \n",
       "\n",
       "           PublishDate  SentimentTitle  SentimentHeadline  Facebook  \\\n",
       "0  2002-04-02 00:00:00             0.0            -0.0533        -1   \n",
       "\n",
       "   GooglePlus  LinkedIn  \n",
       "0          -1        -1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting the data according to the publish date\n",
    "final=final.sort_values(by='PublishDate')\n",
    "final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['PublishDate']=pd.to_datetime(final['PublishDate'])    #converting the data type from category to date format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing records that never entered Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = set(np.where(final['Facebook']==-1)[0])   #index of Facebook = -1\n",
    "temp2 = set(np.where(final['LinkedIn']==-1)[0])     #index of LinkedIn = -1\n",
    "temp3 = set(np.where(final['GooglePlus']==-1)[0])   #index of GooglePlus = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the -1 values with nan\n",
    "final.Facebook.replace(-1,np.nan,inplace=True)\n",
    "final.LinkedIn.replace(-1,np.nan,inplace=True)\n",
    "final.GooglePlus.replace(-1,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping all the nan values if all 3 social media attributes are having -1 \n",
    "final.dropna(thresh=9,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing back the -1 values\n",
    "final.Facebook.replace(np.nan,-1,inplace=True)\n",
    "final.LinkedIn.replace(np.nan,-1,inplace=True)\n",
    "final.GooglePlus.replace(np.nan,-1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDLink                0\n",
       "Title                 0\n",
       "Headline             10\n",
       "Source               38\n",
       "Topic                 0\n",
       "PublishDate           0\n",
       "SentimentTitle        0\n",
       "SentimentHeadline     0\n",
       "Facebook              0\n",
       "GooglePlus            0\n",
       "LinkedIn              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the null columns\n",
    "\n",
    "final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the null indices\n",
    "\n",
    "l1 = list(final[final['Headline'].isnull()].index)\n",
    "l2 = list(final[final['Source'].isnull()].index)\n",
    "s1 = set(l1)\n",
    "s2 = set(l2)\n",
    "len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008069396812588259"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Percentage of null indices compared to the overall size\n",
    "\n",
    "len(s1.union(s2))/final.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can drop the null values after merging it with the six data files - since, we have to check if the final sentiment score of social media columns in final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ESIMasterJournalList-122018.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2a415ca2438e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Reading the journal excel file and appending to an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjournal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ESIMasterJournalList-122018.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Sheet1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mjournal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjournal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Full title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjournal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, **kwds)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     return io.parse(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ESIMasterJournalList-122018.xlsx'"
     ]
    }
   ],
   "source": [
    "#Reading the journal excel file and appending to an empty list \n",
    "journal = pd.read_excel('ESIMasterJournalList-122018.xlsx',sheet_name='Sheet1')\n",
    "journal = journal['Full title'] \n",
    "k = list()\n",
    "for i in journal:\n",
    "    j=str(i).strip()\n",
    "    j=j.lower()\n",
    "    k.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j=0\n",
    "l=list()\n",
    "for i in final['Source']:\n",
    "    i=str(i)\n",
    "    i=i.strip()\n",
    "    i=i.lower()\n",
    "#Categorizing all the online source from source\n",
    "    if(i.find('yahoo') >= 0 or i.find('.gov')>0 or i.find('.fr')>=0):\n",
    "        l.append('Online')\n",
    "    elif(i.find('.com')>0 or i.find('.co')>=0 or i.find('blog')>=0 or i.find('.ca')>=0 or i.find('.tv')>=0 or (i=='bloomberg')):\n",
    "        l.append('Online')\n",
    "#categorizing all the dailies and journals \n",
    "    \n",
    "    elif(i in k):\n",
    "        l.append('Dailies/Journals')\n",
    "    \n",
    "    elif(i.find('today') >= 0 or i.find('times') >= 0 or i.find('daily')>=0 or i.startswith('the')):\n",
    "        l.append('Dailies/Journals')\n",
    "        \n",
    "    elif(i.find('journal')>=0 or i.find('post')>=0 or i.find('magazine')>=0 or i.find('forbes')>=0 or i.find('tass')>=0):\n",
    "        l.append('Dailies/Journals')\n",
    "    elif(i.find('nasdaq')>=0 or i.find('reuters')>=0 or i.find('sputnik')>=0 or i.find('mid-day')>=0 or i.find('quartz')>=0):\n",
    "        l.append('Dailies/Journals')\n",
    "    elif(i.find('herald')>=0 or i.find('market')>=0 or i.find('business')>=0):\n",
    "        l.append('Dailies/Journals')\n",
    "        \n",
    "#Categorizing all the TV news chaneels\n",
    "        \n",
    "    elif(i.find('cnn') >= 0 or i.find('fxstreet')>=0 or i.find('bcc') >= 0 or i.find('cnbc') >= 0 or i.find('news')>=0):\n",
    "        l.append('Television')\n",
    "    elif(i=='time' or i.find('live')>0 or i.find('ndtv')>=0 or i.find('abc') >= 0 or i.find('press')>=0 or (i=='msnbc')):\n",
    "        l.append('Television')\n",
    "    elif(i.find('cbs')>0):\n",
    "        l.append('Television')\n",
    "\n",
    "    else: \n",
    "        l.append('Dailies/Journals')\n",
    "final['S_Source']=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of news items each category has\n",
    "final['S_Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Striping the publish date to find the mont column\n",
    "final['PublishDate'] = final.PublishDate.astype('str')\n",
    "final['month'] = final['PublishDate'].str[0:7]\n",
    "final['month'] = pd.to_datetime(final['month'])\n",
    "final.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Master DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of all the 6 files\n",
    "print(fb_obama.shape)\n",
    "print(fb_economy.shape)\n",
    "print(li_obama.shape)\n",
    "print(li_economy.shape)\n",
    "print(gp_obama.shape)\n",
    "print(gp_economy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different categories of titles \n",
    "fb_obama['sm_title']='fb_obama'\n",
    "fb_economy['sm_title']='fb_economy'\n",
    "li_obama['sm_title']='li_obama'\n",
    "li_economy['sm_title']='li_economy'\n",
    "gp_obama['sm_title']='gp_obama'\n",
    "gp_economy['sm_title']='gp_economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = [fb_obama,fb_economy,li_obama,li_economy,gp_obama,gp_economy]  # cancating all the 6 files\n",
    "result = pd.concat(frame)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.replace(-1,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dropna(thresh=3,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2455/177395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.replace(np.nan,-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the files using the IDLink\n",
    "data = pd.merge(final,result,on='IDLink')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)  #Dropping all the null values of headlines and source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[r['TS144']==-1].groupby(['TS144']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2455/177395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of all the news of economy and obama in the final file\n",
    "final['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of news of economy and obama in the data file\n",
    "data['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#striping of the 3 social media names from sm_title\n",
    "data['sm'] = data['sm_title'].str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count plot\n",
    "sns.countplot('Topic',hue='sm',data=data)\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popularity based on weekday and weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['weekday']=final['month'].apply(lambda x:x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['weekday/weekend']=np.where(final['weekday']<=5,'weekday','weekend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis of title and headlines \n",
    "sentiment_title = final.SentimentTitle\n",
    "sentiment_headline = final.SentimentHeadline\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(sentiment_title)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sentiment Title')\n",
    "plt.title('Timeline Vs SentiTitle')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(sentiment_headline)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sentiment Headline')\n",
    "plt.title('Timeline Vs SentiHeadline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of obama and economy per month\n",
    "print('\\n')\n",
    "for i in final.Topic.unique():\n",
    "    plt.plot(final[final['Topic']==i].groupby(['month']).IDLink.count(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[final['Topic']=='obama'].groupby(['month']).month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[final['Topic']=='economy'].groupby(['month']).month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avarage sentiments of economy and obama\n",
    "print('Sentiment Title\\n')\n",
    "for i in final.Topic.unique():\n",
    "    plt.plot(final[final['Topic']==i].groupby(['month']).SentimentTitle.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Sentiment Title')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Sentiment HeadLine\\n')\n",
    "for i in final.Topic.unique():\n",
    "    plt.plot(final[final['Topic']==i].groupby(['month']).SentimentHeadline.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Sentiment Headline')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1- Average unique topics in facebook for each month\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    plt.plot(data[data['Topic']==i].groupby(['month']).Facebook.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Facebook')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2- Average unique topics in LinkedIn for each month\n",
    "\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    plt.plot(data[data['Topic']==i].groupby(['month']).LinkedIn.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('LinkedIn')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plot 3- Average unique topics in GooglePlus for each month\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    plt.plot(data[data['Topic']==i].groupby(['month']).GooglePlus.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Google Plus')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook = [fb_obama,fb_economy]   #concating all the items according to the social platform\n",
    "facebook = pd.concat(facebook)\n",
    "\n",
    "linkedin = [li_obama,li_economy]\n",
    "linkedin = pd.concat(linkedin)\n",
    "\n",
    "gplus = [gp_obama,gp_economy]\n",
    "googleplus = pd.concat(gplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot-1 The grap explains the time span the news took to enter social media platform\n",
    "print('Facebook:\\n\\n')\n",
    "\n",
    "columns = [i for i in facebook if i not in ('IDLink','sm_title')]\n",
    "l1 = list()\n",
    "l2 = list()\n",
    "l3 = range(1,145)\n",
    "for i in columns:\n",
    "    t_ob=np.mean(fb_obama[i])\n",
    "    t_ec=np.mean(fb_economy[i])\n",
    "    l1.append(t_ob/np.mean(fb_obama['TS144']))\n",
    "    l2.append(t_ec/np.mean(fb_economy['TS144']))\n",
    "plt.plot(l3,l1,label='obama')\n",
    "plt.plot(l3,l2,label='economy')\n",
    "plt.title('Facebook')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('LinkedIN:\\n\\n')\n",
    "\n",
    "columns = [i for i in linkedin if i not in ('IDLink','sm_title')]\n",
    "l1 = list()\n",
    "l2 = list()\n",
    "l3 = range(1,145)\n",
    "for i in columns:\n",
    "    t_ob=np.mean(li_obama[i])\n",
    "    t_ec=np.mean(li_economy[i])\n",
    "    l1.append(t_ob/np.mean(li_obama['TS144']))\n",
    "    l2.append(t_ec/np.mean(li_economy['TS144']))\n",
    "plt.plot(l3,l1,label='obama')\n",
    "plt.plot(l3,l2,label='economy')\n",
    "plt.title('LinkedIn')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Google Plus:\\n\\n')\n",
    "\n",
    "columns = [i for i in googleplus if i not in ('IDLink','sm_title')]\n",
    "l1 = list()\n",
    "l2 = list()\n",
    "l3 = range(1,145)\n",
    "for i in columns:\n",
    "    t_ob=np.mean(gp_obama[i])\n",
    "    t_ec=np.mean(gp_economy[i])\n",
    "    l1.append(t_ob/np.mean(gp_obama['TS144']))\n",
    "    l2.append(t_ec/np.mean(gp_economy['TS144']))\n",
    "plt.plot(l3,l1,label='obama')\n",
    "plt.plot(l3,l2,label='economy')\n",
    "plt.title('Google Plus')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The plots demonstrates the positive and negative score of Economy and Obama in 3 different social media platform\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    temp=data[data['Topic']==i]\n",
    "    \n",
    "    print(i.upper(),':')\n",
    "    for j in temp.sm.unique():\n",
    "        print(j.upper(),':')\n",
    "        t = temp[temp['sm']==j]\n",
    "        temp_pos=t[t['SentimentTitle']>=0]\n",
    "        temp_neg=t[t['SentimentTitle']<0]\n",
    "        \n",
    "        f , ax_arr = plt.subplots(1 , 2 , figsize=(8,4),sharex=True)\n",
    "        \n",
    "        ax_arr[0].hist(temp_pos.groupby('month')['SentimentTitle'].mean(),align='mid',histtype='step')\n",
    "        ax_arr[0].set_title('Positive')\n",
    "        ax_arr[0].set_xlabel('Frequency')\n",
    "        ax_arr[0].set_xlabel('Sentiment Title')\n",
    "        \n",
    "    \n",
    "        ax_arr[1].hist(temp_neg.groupby('month')['SentimentTitle'].mean())\n",
    "        ax_arr[1].set_title('Negative')\n",
    "        ax_arr[1].set_xlabel('Frequency')\n",
    "        ax_arr[1].set_xlabel('Sentiment Title')\n",
    "        \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive'] = np.where(round(df['SentimentTitle'])<0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "df.dropna(inplace=True)\n",
    "def get_sentiment(review):\n",
    "    compound = sentiment.polarity_scores(review)['compound']\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['vaderTitle']=df['Title'].apply(get_sentiment)\n",
    "\n",
    "accuracy_score(df['Positive'],df['vaderTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(review):\n",
    "    compound = TextBlob(review).sentiment[0]\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['textblobTitle']=df['Title'].apply(get_sentiment)\n",
    "accuracy_score(df['Positive'],df['textblobTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive_HL'] = np.where(round(df['SentimentHeadline'])<0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "def get_sentiment(review):\n",
    "    compound = sentiment.polarity_scores(review)['compound']\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['vaderHL']=df['Headline'].apply(get_sentiment)\n",
    "\n",
    "accuracy_score(df['Positive_HL'],df['vaderHL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(review):\n",
    "    compound = TextBlob(review).sentiment[0]\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['textblobHL']=df['Headline'].apply(get_sentiment)\n",
    "\n",
    "accuracy_score(df['Positive_HL'],df['textblobHL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "def get_sentiment(review):\n",
    "    compound = sentiment.polarity_scores(review)['compound']\n",
    "    return compound\n",
    "    \n",
    "df['vaderHL']=df['Headline'].apply(get_sentiment)\n",
    "df['vaderTitle']=df['Title'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(review):\n",
    "    compound = TextBlob(review).sentiment[0]\n",
    "    return compound\n",
    "    \n",
    "df['textblobTitle']=df['Title'].apply(get_sentiment)\n",
    "df['textblobHL']=df['Headline'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obama = df[df['Topic'] == 'obama']\n",
    "df_economy = df[df['Topic'] == 'economy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus\n",
    "\n",
    "docs = df_obama.Title.str.lower().str.replace('[^a-z\\' ]','')\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stopwords.extend(['obama','barack','say','call','new'])\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def clean_sen(text):\n",
    "    text=str(text)\n",
    "    ws = text.split(' ')\n",
    "    ws = [w.strip() for w in ws]\n",
    "    ws_clean = [w for w in ws if w not in stopwords]\n",
    "    return ' '.join(ws_clean)\n",
    "\n",
    "docs_clean = docs.apply(clean_sen)\n",
    "\n",
    "#Merging all paragraphs as a single string\n",
    "text = ' '.join(para for para in docs_clean)\n",
    "\n",
    "#Creating the word cloud\n",
    "plt.figure(figsize=(10,10))\n",
    "wc = WordCloud().generate(text)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus\n",
    "\n",
    "docs = df_economy.Title.str.lower().str.replace('[^a-z\\' ]','')\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stopwords.extend(['economy'])\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def clean_sen(text):\n",
    "    text=str(text)\n",
    "    ws = text.split(' ')\n",
    "    ws_clean = [w for w in ws if w not in stopwords]\n",
    "    return ' '.join(ws_clean)\n",
    "\n",
    "docs_clean = docs.apply(clean_sen)\n",
    "\n",
    "#Merging all paragraphs as a single string\n",
    "text = ' '.join(para for para in docs_clean)\n",
    "\n",
    "#Creating the word cloud\n",
    "plt.figure(figsize=(10,10))\n",
    "wc = WordCloud().generate(text)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_title = df['Title'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "docs_headline = df['Headline'].fillna('').str.lower().str.replace('[^a-z ]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend([])\n",
    "\n",
    "docs_title_clean = []\n",
    "for doc in docs_title:\n",
    "    words = doc.split(' ')\n",
    "    wc = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_title_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend([])\n",
    "\n",
    "docs_headline_clean = []\n",
    "for doc in docs_headline:\n",
    "    words = doc.split(' ')\n",
    "    wc = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_headline_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "dictionary_t = gensim.corpora.Dictionary(docs_title_clean)\n",
    "dictionary_hl = gensim.corpora.Dictionary(docs_headline_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow_title = []\n",
    "for doc in docs_title_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_title.append(bow)\n",
    "\n",
    "docs_bow_headline = []\n",
    "for doc in docs_headline_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_headline.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scores = []\n",
    "for i in range(4,20):\n",
    "    lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=i,random_state=100)\n",
    "    coher_model = CoherenceModel(lda_model,corpus=docs_bow_title,coherence='u_mass')\n",
    "    score = coher_model.get_coherence()\n",
    "    c_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=list(range(4,20))\n",
    "d=dict()\n",
    "j=0\n",
    "for i in l1:\n",
    "    d[i]=c_scores[j]\n",
    "    j=j+1\n",
    "sns.lineplot(l1,c_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=5,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(docs_bow_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(lda_model.get_document_topics(docs_bow_title[1]),columns=['Topic','Probablity'])\n",
    "topics_df.sort_values(by='Probablity').iloc[:]['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['Topic']=='obama']\n",
    "\n",
    "docs_title = temp['Title'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "docs_headline = temp['Headline'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['obama','','barack','say','call','new','obamas','u'])\n",
    "\n",
    "docs_title_clean = []\n",
    "for doc in docs_title:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_title_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['obama','','barack','say','call','new','obamas','u'])\n",
    "\n",
    "docs_headline_clean = []\n",
    "for doc in docs_headline:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_headline_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_t = gensim.corpora.Dictionary(docs_title_clean)\n",
    "dictionary_hl = gensim.corpora.Dictionary(docs_headline_clean)\n",
    "\n",
    "docs_bow_title = []\n",
    "for doc in docs_title_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_title.append(bow)\n",
    "\n",
    "docs_bow_headline = []\n",
    "for doc in docs_headline_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_headline.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scores = []\n",
    "for i in range(1,10 ):\n",
    "    lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=i,random_state=100)\n",
    "    coher_model = CoherenceModel(lda_model,corpus=docs_bow_title,coherence='u_mass')\n",
    "    score = coher_model.get_coherence()\n",
    "    c_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=list(range(1,10))\n",
    "d=dict()\n",
    "j=0\n",
    "for i in l1:\n",
    "    d[i]=c_scores[j]\n",
    "    j=j+1\n",
    "sns.lineplot(l1,c_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=2,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(docs_bow_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(lda_model.get_document_topics(docs_bow_title[1]),columns=['Topic','Probablity'])\n",
    "topics_df.sort_values(by='Probablity').iloc[:]['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling for Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_eco = df[df['Topic']=='economy']\n",
    "\n",
    "docs_title = temp_eco['Title'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "docs_headline = temp_eco['Headline'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['economy','us','say','would','could','new',''])\n",
    "\n",
    "docs_title_clean = []\n",
    "for doc in docs_title:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_title_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['economy','us','say','would','could','new',''])\n",
    "\n",
    "docs_headline_clean = []\n",
    "for doc in docs_headline:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_headline_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_e = gensim.corpora.Dictionary(docs_title_clean)\n",
    "dictionary_eco = gensim.corpora.Dictionary(docs_headline_clean)\n",
    "\n",
    "docs_bow_title = []\n",
    "for doc in docs_title_clean:\n",
    "    bow = dictionary_e.doc2bow(doc)\n",
    "    docs_bow_title.append(bow)\n",
    "\n",
    "docs_bow_headline = []\n",
    "for doc in docs_headline_clean:\n",
    "    bow = dictionary_e.doc2bow(doc)\n",
    "    docs_bow_headline.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scores = []\n",
    "for i in range(1,10 ):\n",
    "    lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_e,num_topics=i,random_state=100)\n",
    "    coher_model = CoherenceModel(lda_model,corpus=docs_bow_title,coherence='u_mass')\n",
    "    score = coher_model.get_coherence()\n",
    "    c_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=list(range(1,10))\n",
    "d=dict()\n",
    "j=0\n",
    "for i in l1:\n",
    "    d[i]=c_scores[j]\n",
    "    j=j+1\n",
    "sns.lineplot(l1,c_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_e,num_topics=5,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(docs_bow_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(lda_model.get_document_topics(docs_bow_title[1]),columns=['Topic','Probablity'])\n",
    "topics_df.sort_values(by='Probablity').iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.SentimentHeadline,df.textblobHL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.SentimentHeadline,df.vaderHL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.SentimentHeadline,df.Facebook)\n",
    "plt.title('Facebook')\n",
    "plt.xlabel('Sentiment Headline')\n",
    "plt.ylabel('Popularity')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df.SentimentHeadline,df.LinkedIn)\n",
    "plt.title('LinkedIn')\n",
    "plt.xlabel('Sentiment Headline')\n",
    "plt.ylabel('Popularity')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df.SentimentHeadline,df.GooglePlus)\n",
    "plt.title('Google Plus')\n",
    "plt.xlabel('Sentiment Headline')\n",
    "plt.ylabel('Popularity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
