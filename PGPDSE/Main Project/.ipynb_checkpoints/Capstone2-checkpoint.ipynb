{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cbca20a775ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "#Basic Libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Text Mining\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "\n",
    "#Topic Modelling\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Others\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Files\n",
    "\n",
    "fb_obama = pd.read_csv('Facebook_Obama.csv')\n",
    "gp_obama = pd.read_csv('GooglePlus_Obama.csv')\n",
    "li_obama = pd.read_csv('LinkedIn_Obama.csv')\n",
    "\n",
    "fb_economy = pd.read_csv('Facebook_Economy.csv')\n",
    "gp_economy = pd.read_csv('GooglePlus_Economy.csv')\n",
    "li_economy = pd.read_csv('LinkedIn_Economy.csv')\n",
    "\n",
    "final = pd.read_csv('News_Final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Palestine and Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data based on the topics required\n",
    "temp1 = np.where((final['Topic']=='palestine'))[0]   #index of Palestine\n",
    "temp2 = np.where(final['Topic']=='microsoft')[0]     #index of microsoft\n",
    "print(len(temp1)+len(temp2))\n",
    "final.shape #initial shape  -- final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.drop(temp1,axis=0,inplace=True)   #Dropping Palestine\n",
    "final.drop(temp2,axis=0,inplace=True)   #Dropping Microsoft\n",
    "final.shape #final shape  -- final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the data according to the publish date\n",
    "final=final.sort_values(by='PublishDate')\n",
    "final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['PublishDate']=pd.to_datetime(final['PublishDate'])    #converting the data type from category to date format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing records that never entered Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = set(np.where(final['Facebook']==-1)[0])   #index of Facebook = -1\n",
    "temp2 = set(np.where(final['LinkedIn']==-1)[0])     #index of LinkedIn = -1\n",
    "temp3 = set(np.where(final['GooglePlus']==-1)[0])   #index of GooglePlus = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the -1 values with nan\n",
    "final.Facebook.replace(-1,np.nan,inplace=True)\n",
    "final.LinkedIn.replace(-1,np.nan,inplace=True)\n",
    "final.GooglePlus.replace(-1,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping all the nan values if all 3 social media attributes are having -1 \n",
    "final.dropna(thresh=9,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing back the -1 values\n",
    "final.Facebook.replace(np.nan,-1,inplace=True)\n",
    "final.LinkedIn.replace(np.nan,-1,inplace=True)\n",
    "final.GooglePlus.replace(np.nan,-1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying the null columns\n",
    "\n",
    "final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying the null indices\n",
    "\n",
    "l1 = list(final[final['Headline'].isnull()].index)\n",
    "l2 = list(final[final['Source'].isnull()].index)\n",
    "s1 = set(l1)\n",
    "s2 = set(l2)\n",
    "len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of null indices compared to the overall size\n",
    "\n",
    "len(s1.union(s2))/final.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can drop the null values after merging it with the six data files - since, we have to check if the final sentiment score of social media columns in final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the journal excel file and appending to an empty list \n",
    "journal = pd.read_excel('ESIMasterJournalList-122018.xlsx',sheet_name='Sheet1')\n",
    "journal = journal['Full title'] \n",
    "k = list()\n",
    "for i in journal:\n",
    "    j=str(i).strip()\n",
    "    j=j.lower()\n",
    "    k.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j=0\n",
    "l=list()\n",
    "for i in final['Source']:\n",
    "    i=str(i)\n",
    "    i=i.strip()\n",
    "    i=i.lower()\n",
    "#Categorizing all the online source from source\n",
    "    if(i.find('yahoo') >= 0 or i.find('.gov')>0 or i.find('.fr')>=0):\n",
    "        l.append('Online')\n",
    "    elif(i.find('.com')>0 or i.find('.co')>=0 or i.find('blog')>=0 or i.find('.ca')>=0 or i.find('.tv')>=0 or (i=='bloomberg')):\n",
    "        l.append('Online')\n",
    "#categorizing all the dailies and journals \n",
    "    \n",
    "    elif(i in k):\n",
    "        l.append('Dailies/Journals')\n",
    "    \n",
    "    elif(i.find('today') >= 0 or i.find('times') >= 0 or i.find('daily')>=0 or i.startswith('the')):\n",
    "        l.append('Dailies/Journals')\n",
    "        \n",
    "    elif(i.find('journal')>=0 or i.find('post')>=0 or i.find('magazine')>=0 or i.find('forbes')>=0 or i.find('tass')>=0):\n",
    "        l.append('Dailies/Journals')\n",
    "    elif(i.find('nasdaq')>=0 or i.find('reuters')>=0 or i.find('sputnik')>=0 or i.find('mid-day')>=0 or i.find('quartz')>=0):\n",
    "        l.append('Dailies/Journals')\n",
    "    elif(i.find('herald')>=0 or i.find('market')>=0 or i.find('business')>=0):\n",
    "        l.append('Dailies/Journals')\n",
    "        \n",
    "#Categorizing all the TV news chaneels\n",
    "        \n",
    "    elif(i.find('cnn') >= 0 or i.find('fxstreet')>=0 or i.find('bcc') >= 0 or i.find('cnbc') >= 0 or i.find('news')>=0):\n",
    "        l.append('Television')\n",
    "    elif(i=='time' or i.find('live')>0 or i.find('ndtv')>=0 or i.find('abc') >= 0 or i.find('press')>=0 or (i=='msnbc')):\n",
    "        l.append('Television')\n",
    "    elif(i.find('cbs')>0):\n",
    "        l.append('Television')\n",
    "\n",
    "    else: \n",
    "        l.append('Dailies/Journals')\n",
    "final['S_Source']=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of news items each category has\n",
    "final['S_Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Striping the publish date to find the mont column\n",
    "final['PublishDate'] = final.PublishDate.astype('str')\n",
    "final['month'] = final['PublishDate'].str[0:7]\n",
    "final['month'] = pd.to_datetime(final['month'])\n",
    "final.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Master DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of all the 6 files\n",
    "print(fb_obama.shape)\n",
    "print(fb_economy.shape)\n",
    "print(li_obama.shape)\n",
    "print(li_economy.shape)\n",
    "print(gp_obama.shape)\n",
    "print(gp_economy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different categories of titles \n",
    "fb_obama['sm_title']='fb_obama'\n",
    "fb_economy['sm_title']='fb_economy'\n",
    "li_obama['sm_title']='li_obama'\n",
    "li_economy['sm_title']='li_economy'\n",
    "gp_obama['sm_title']='gp_obama'\n",
    "gp_economy['sm_title']='gp_economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = [fb_obama,fb_economy,li_obama,li_economy,gp_obama,gp_economy]  # cancating all the 6 files\n",
    "result = pd.concat(frame)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.replace(-1,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dropna(thresh=3,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2455/177395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.replace(np.nan,-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the files using the IDLink\n",
    "data = pd.merge(final,result,on='IDLink')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)  #Dropping all the null values of headlines and source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[r['TS144']==-1].groupby(['TS144']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2455/177395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of all the news of economy and obama in the final file\n",
    "final['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of news of economy and obama in the data file\n",
    "data['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#striping of the 3 social media names from sm_title\n",
    "data['sm'] = data['sm_title'].str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count plot\n",
    "sns.countplot('Topic',hue='sm',data=data)\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popularity based on weekday and weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['weekday']=final['month'].apply(lambda x:x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['weekday/weekend']=np.where(final['weekday']<=5,'weekday','weekend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis of title and headlines \n",
    "sentiment_title = final.SentimentTitle\n",
    "sentiment_headline = final.SentimentHeadline\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(sentiment_title)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sentiment Title')\n",
    "plt.title('Timeline Vs SentiTitle')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(sentiment_headline)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sentiment Headline')\n",
    "plt.title('Timeline Vs SentiHeadline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of obama and economy per month\n",
    "print('\\n')\n",
    "for i in final.Topic.unique():\n",
    "    plt.plot(final[final['Topic']==i].groupby(['month']).IDLink.count(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[final['Topic']=='obama'].groupby(['month']).month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[final['Topic']=='economy'].groupby(['month']).month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avarage sentiments of economy and obama\n",
    "print('Sentiment Title\\n')\n",
    "for i in final.Topic.unique():\n",
    "    plt.plot(final[final['Topic']==i].groupby(['month']).SentimentTitle.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Sentiment Title')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Sentiment HeadLine\\n')\n",
    "for i in final.Topic.unique():\n",
    "    plt.plot(final[final['Topic']==i].groupby(['month']).SentimentHeadline.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Sentiment Headline')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1- Average unique topics in facebook for each month\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    plt.plot(data[data['Topic']==i].groupby(['month']).Facebook.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Facebook')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2- Average unique topics in LinkedIn for each month\n",
    "\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    plt.plot(data[data['Topic']==i].groupby(['month']).LinkedIn.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('LinkedIn')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plot 3- Average unique topics in GooglePlus for each month\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    plt.plot(data[data['Topic']==i].groupby(['month']).GooglePlus.mean(),label=i)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Google Plus')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook = [fb_obama,fb_economy]   #concating all the items according to the social platform\n",
    "facebook = pd.concat(facebook)\n",
    "\n",
    "linkedin = [li_obama,li_economy]\n",
    "linkedin = pd.concat(linkedin)\n",
    "\n",
    "gplus = [gp_obama,gp_economy]\n",
    "googleplus = pd.concat(gplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot-1 The grap explains the time span the news took to enter social media platform\n",
    "print('Facebook:\\n\\n')\n",
    "\n",
    "columns = [i for i in facebook if i not in ('IDLink','sm_title')]\n",
    "l1 = list()\n",
    "l2 = list()\n",
    "l3 = range(1,145)\n",
    "for i in columns:\n",
    "    t_ob=np.mean(fb_obama[i])\n",
    "    t_ec=np.mean(fb_economy[i])\n",
    "    l1.append(t_ob/np.mean(fb_obama['TS144']))\n",
    "    l2.append(t_ec/np.mean(fb_economy['TS144']))\n",
    "plt.plot(l3,l1,label='obama')\n",
    "plt.plot(l3,l2,label='economy')\n",
    "plt.title('Facebook')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('LinkedIN:\\n\\n')\n",
    "\n",
    "columns = [i for i in linkedin if i not in ('IDLink','sm_title')]\n",
    "l1 = list()\n",
    "l2 = list()\n",
    "l3 = range(1,145)\n",
    "for i in columns:\n",
    "    t_ob=np.mean(li_obama[i])\n",
    "    t_ec=np.mean(li_economy[i])\n",
    "    l1.append(t_ob/np.mean(li_obama['TS144']))\n",
    "    l2.append(t_ec/np.mean(li_economy['TS144']))\n",
    "plt.plot(l3,l1,label='obama')\n",
    "plt.plot(l3,l2,label='economy')\n",
    "plt.title('LinkedIn')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Google Plus:\\n\\n')\n",
    "\n",
    "columns = [i for i in googleplus if i not in ('IDLink','sm_title')]\n",
    "l1 = list()\n",
    "l2 = list()\n",
    "l3 = range(1,145)\n",
    "for i in columns:\n",
    "    t_ob=np.mean(gp_obama[i])\n",
    "    t_ec=np.mean(gp_economy[i])\n",
    "    l1.append(t_ob/np.mean(gp_obama['TS144']))\n",
    "    l2.append(t_ec/np.mean(gp_economy['TS144']))\n",
    "plt.plot(l3,l1,label='obama')\n",
    "plt.plot(l3,l2,label='economy')\n",
    "plt.title('Google Plus')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The plots demonstrates the positive and negative score of Economy and Obama in 3 different social media platform\n",
    "\n",
    "for i in data.Topic.unique():\n",
    "    temp=data[data['Topic']==i]\n",
    "    \n",
    "    print(i.upper(),':')\n",
    "    for j in temp.sm.unique():\n",
    "        print(j.upper(),':')\n",
    "        t = temp[temp['sm']==j]\n",
    "        temp_pos=t[t['SentimentTitle']>=0]\n",
    "        temp_neg=t[t['SentimentTitle']<0]\n",
    "        \n",
    "        f , ax_arr = plt.subplots(1 , 2 , figsize=(8,4),sharex=True)\n",
    "        \n",
    "        ax_arr[0].hist(temp_pos.groupby('month')['SentimentTitle'].mean(),align='mid',histtype='step')\n",
    "        ax_arr[0].set_title('Positive')\n",
    "        ax_arr[0].set_xlabel('Frequency')\n",
    "        ax_arr[0].set_xlabel('Sentiment Title')\n",
    "        \n",
    "    \n",
    "        ax_arr[1].hist(temp_neg.groupby('month')['SentimentTitle'].mean())\n",
    "        ax_arr[1].set_title('Negative')\n",
    "        ax_arr[1].set_xlabel('Frequency')\n",
    "        ax_arr[1].set_xlabel('Sentiment Title')\n",
    "        \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive'] = np.where(round(df['SentimentTitle'])<0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "df.dropna(inplace=True)\n",
    "def get_sentiment(review):\n",
    "    compound = sentiment.polarity_scores(review)['compound']\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['vaderTitle']=df['Title'].apply(get_sentiment)\n",
    "\n",
    "accuracy_score(df['Positive'],df['vaderTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(review):\n",
    "    compound = TextBlob(review).sentiment[0]\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['textblobTitle']=df['Title'].apply(get_sentiment)\n",
    "accuracy_score(df['Positive'],df['textblobTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive_HL'] = np.where(round(df['SentimentHeadline'])<0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "def get_sentiment(review):\n",
    "    compound = sentiment.polarity_scores(review)['compound']\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['vaderHL']=df['Headline'].apply(get_sentiment)\n",
    "\n",
    "accuracy_score(df['Positive_HL'],df['vaderHL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(review):\n",
    "    compound = TextBlob(review).sentiment[0]\n",
    "    if compound<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['textblobHL']=df['Headline'].apply(get_sentiment)\n",
    "\n",
    "accuracy_score(df['Positive_HL'],df['textblobHL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "def get_sentiment(review):\n",
    "    compound = sentiment.polarity_scores(review)['compound']\n",
    "    return compound\n",
    "    \n",
    "df['vaderHL']=df['Headline'].apply(get_sentiment)\n",
    "df['vaderTitle']=df['Title'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(review):\n",
    "    compound = TextBlob(review).sentiment[0]\n",
    "    return compound\n",
    "    \n",
    "df['textblobTitle']=df['Title'].apply(get_sentiment)\n",
    "df['textblobHL']=df['Headline'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obama = df[df['Topic'] == 'obama']\n",
    "df_economy = df[df['Topic'] == 'economy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus\n",
    "\n",
    "docs = df_obama.Title.str.lower().str.replace('[^a-z\\' ]','')\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stopwords.extend(['obama','barack','say','call','new'])\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def clean_sen(text):\n",
    "    text=str(text)\n",
    "    ws = text.split(' ')\n",
    "    ws = [w.strip() for w in ws]\n",
    "    ws_clean = [w for w in ws if w not in stopwords]\n",
    "    return ' '.join(ws_clean)\n",
    "\n",
    "docs_clean = docs.apply(clean_sen)\n",
    "\n",
    "#Merging all paragraphs as a single string\n",
    "text = ' '.join(para for para in docs_clean)\n",
    "\n",
    "#Creating the word cloud\n",
    "plt.figure(figsize=(10,10))\n",
    "wc = WordCloud().generate(text)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus\n",
    "\n",
    "docs = df_economy.Title.str.lower().str.replace('[^a-z\\' ]','')\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stopwords.extend(['economy'])\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def clean_sen(text):\n",
    "    text=str(text)\n",
    "    ws = text.split(' ')\n",
    "    ws_clean = [w for w in ws if w not in stopwords]\n",
    "    return ' '.join(ws_clean)\n",
    "\n",
    "docs_clean = docs.apply(clean_sen)\n",
    "\n",
    "#Merging all paragraphs as a single string\n",
    "text = ' '.join(para for para in docs_clean)\n",
    "\n",
    "#Creating the word cloud\n",
    "plt.figure(figsize=(10,10))\n",
    "wc = WordCloud().generate(text)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_title = df['Title'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "docs_headline = df['Headline'].fillna('').str.lower().str.replace('[^a-z ]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend([])\n",
    "\n",
    "docs_title_clean = []\n",
    "for doc in docs_title:\n",
    "    words = doc.split(' ')\n",
    "    wc = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_title_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend([])\n",
    "\n",
    "docs_headline_clean = []\n",
    "for doc in docs_headline:\n",
    "    words = doc.split(' ')\n",
    "    wc = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_headline_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "dictionary_t = gensim.corpora.Dictionary(docs_title_clean)\n",
    "dictionary_hl = gensim.corpora.Dictionary(docs_headline_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow_title = []\n",
    "for doc in docs_title_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_title.append(bow)\n",
    "\n",
    "docs_bow_headline = []\n",
    "for doc in docs_headline_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_headline.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scores = []\n",
    "for i in range(4,20):\n",
    "    lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=i,random_state=100)\n",
    "    coher_model = CoherenceModel(lda_model,corpus=docs_bow_title,coherence='u_mass')\n",
    "    score = coher_model.get_coherence()\n",
    "    c_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=list(range(4,20))\n",
    "d=dict()\n",
    "j=0\n",
    "for i in l1:\n",
    "    d[i]=c_scores[j]\n",
    "    j=j+1\n",
    "sns.lineplot(l1,c_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=5,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(docs_bow_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(lda_model.get_document_topics(docs_bow_title[1]),columns=['Topic','Probablity'])\n",
    "topics_df.sort_values(by='Probablity').iloc[:]['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['Topic']=='obama']\n",
    "\n",
    "docs_title = temp['Title'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "docs_headline = temp['Headline'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['obama','','barack','say','call','new','obamas','u'])\n",
    "\n",
    "docs_title_clean = []\n",
    "for doc in docs_title:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_title_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['obama','','barack','say','call','new','obamas','u'])\n",
    "\n",
    "docs_headline_clean = []\n",
    "for doc in docs_headline:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_headline_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_t = gensim.corpora.Dictionary(docs_title_clean)\n",
    "dictionary_hl = gensim.corpora.Dictionary(docs_headline_clean)\n",
    "\n",
    "docs_bow_title = []\n",
    "for doc in docs_title_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_title.append(bow)\n",
    "\n",
    "docs_bow_headline = []\n",
    "for doc in docs_headline_clean:\n",
    "    bow = dictionary_t.doc2bow(doc)\n",
    "    docs_bow_headline.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scores = []\n",
    "for i in range(1,10 ):\n",
    "    lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=i,random_state=100)\n",
    "    coher_model = CoherenceModel(lda_model,corpus=docs_bow_title,coherence='u_mass')\n",
    "    score = coher_model.get_coherence()\n",
    "    c_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=list(range(1,10))\n",
    "d=dict()\n",
    "j=0\n",
    "for i in l1:\n",
    "    d[i]=c_scores[j]\n",
    "    j=j+1\n",
    "sns.lineplot(l1,c_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_t,num_topics=2,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(docs_bow_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(lda_model.get_document_topics(docs_bow_title[1]),columns=['Topic','Probablity'])\n",
    "topics_df.sort_values(by='Probablity').iloc[:]['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling for Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_eco = df[df['Topic']=='economy']\n",
    "\n",
    "docs_title = temp_eco['Title'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "docs_headline = temp_eco['Headline'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['economy','us','say','would','could','new',''])\n",
    "\n",
    "docs_title_clean = []\n",
    "for doc in docs_title:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_title_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords.extend(['economy','us','say','would','could','new',''])\n",
    "\n",
    "docs_headline_clean = []\n",
    "for doc in docs_headline:\n",
    "    words = doc.split(' ')\n",
    "    wc = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    wc = [word for word in wc if word not in stopwords]\n",
    "    docs_headline_clean.append(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_e = gensim.corpora.Dictionary(docs_title_clean)\n",
    "dictionary_eco = gensim.corpora.Dictionary(docs_headline_clean)\n",
    "\n",
    "docs_bow_title = []\n",
    "for doc in docs_title_clean:\n",
    "    bow = dictionary_e.doc2bow(doc)\n",
    "    docs_bow_title.append(bow)\n",
    "\n",
    "docs_bow_headline = []\n",
    "for doc in docs_headline_clean:\n",
    "    bow = dictionary_e.doc2bow(doc)\n",
    "    docs_bow_headline.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scores = []\n",
    "for i in range(1,10 ):\n",
    "    lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_e,num_topics=i,random_state=100)\n",
    "    coher_model = CoherenceModel(lda_model,corpus=docs_bow_title,coherence='u_mass')\n",
    "    score = coher_model.get_coherence()\n",
    "    c_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=list(range(1,10))\n",
    "d=dict()\n",
    "j=0\n",
    "for i in l1:\n",
    "    d[i]=c_scores[j]\n",
    "    j=j+1\n",
    "sns.lineplot(l1,c_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(docs_bow_title,id2word=dictionary_e,num_topics=5,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(docs_bow_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(lda_model.get_document_topics(docs_bow_title[1]),columns=['Topic','Probablity'])\n",
    "topics_df.sort_values(by='Probablity').iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.SentimentHeadline,df.textblobHL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.SentimentHeadline,df.vaderHL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.SentimentHeadline,df.Facebook)\n",
    "plt.title('Facebook')\n",
    "plt.xlabel('Sentiment Headline')\n",
    "plt.ylabel('Popularity')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df.SentimentHeadline,df.LinkedIn)\n",
    "plt.title('LinkedIn')\n",
    "plt.xlabel('Sentiment Headline')\n",
    "plt.ylabel('Popularity')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df.SentimentHeadline,df.GooglePlus)\n",
    "plt.title('Google Plus')\n",
    "plt.xlabel('Sentiment Headline')\n",
    "plt.ylabel('Popularity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
